{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8107bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afa1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input_file_path = sys.argv[1]\n",
    "input_test_file_path = sys.argv[2]\n",
    "output_file_path = sys.argv[3]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "input_folder_path = \"/Users/gopi/Desktop/Project/\"\n",
    "input_test_file_path = \"/Users/gopi/Desktop/Assignment3/yelp_val_in.csv\"\n",
    "output_file_path = \"/Users/gopi/Desktop/Project/output_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916418d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/10 14:55:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"project\").set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"4g\").set(\"spark.ui.port\", \"27017\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8eb4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "yelp_categories = [\"Active Life\", \"Arts & Entertainment\", \"Automotive\", \"Bars\", \"Beauty & Spas\", \"Education\", \"Event Planning & Services\", \n",
    "\"Financial Services\", \"Food\", \"Health & Medical\", \"Home Services\", \"Hotels & Travel\", \"Local Flavor\", \"Local Services\", \"Mass Media\", \n",
    "\"Nightlife\", \"Pets\", \"Professional Services\", \"Public Services & Government\", \"Real Estate\", \"Religious Organizations\", \"Restaurants\", \"Shopping\"]\n",
    "\n",
    "print(len(yelp_categories))\n",
    "cat_map = {}\n",
    "for i,cat in enumerate(yelp_categories):\n",
    "    cat_map[cat] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2ee6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass Media\n"
     ]
    }
   ],
   "source": [
    "print(yelp_categories[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3da5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer= SentimentIntensityAnalyzer()\n",
    "def filter_users(user_id) :\n",
    "    return user_id in user_id_set\n",
    "\n",
    "def filter_business(business_id):\n",
    "    return business_id in business_id_set\n",
    "\n",
    "def load_user_data(user):\n",
    "    result = [user['user_id'], user['review_count'], user['average_stars'], user.get('fans',0), user.get('useful', 0),user.get('cool',0),user.get('funny',0), int(user.get('yelping_since')[2:4]), user.get('friends','').count(','), user.get('elite','').count(','), user.get('compliment_hot',0), user.get('compliment_photos',0),user.get('compliment_writer',0),user.get('compliment_funny',0),user.get('compliment_cool',0)]\n",
    "    return tuple(result)\n",
    "\n",
    "def load_business_data(business):\n",
    "    result = [business['business_id'], business['review_count'], business['stars'], business.get('latitude', 0), business.get('longitude',0), business.get('is_open',0), photo_count_business.get(business['business_id'],0), business[\"neighborhood\"], business['categories']]\n",
    "    return tuple(result)\n",
    "\n",
    "def get_final_data(data):\n",
    "    global bus_nf_count\n",
    "    global usr_nf_count\n",
    "    global bus_in_count\n",
    "    global usr_in_count\n",
    "    time_1 = time.time()\n",
    "    user_id = data[0]\n",
    "    business_id = data[1]\n",
    "    \n",
    "    final_data = []\n",
    "    if user_id in user_data:\n",
    "        user_id_data = user_data[user_id]\n",
    "        user_id_data = list(user_id_data)\n",
    "    elif user_id in user_avg_from_train:\n",
    "        user_id_data = [user_avg_from_train[user_id][0], user_avg_from_train[user_id][1], 0,0,0,0,0,0,0,0]\n",
    "        usr_in_count += 1\n",
    "    else:\n",
    "        usr_nf_count += 1\n",
    "        user_id_data = [0 for i in range(10)]\n",
    "    \n",
    "    if business_id in business_data:\n",
    "        business_id_data = business_data[business_id]\n",
    "    elif business_id in business_avg_from_train:\n",
    "        business_id_data = [business_avg_from_train[business_id][0], business_avg_from_train[business_id][1]]\n",
    "        empty = [0 for i in range(42)]\n",
    "        business_id_data.extend(empty)\n",
    "        bus_in_count += 1\n",
    "    else :\n",
    "        bus_nf_count += 1\n",
    "        business_id_data = [0 for i in range(44)]\n",
    "    \n",
    "    if(len(business_id_data)) != 44:\n",
    "        business_id_data = list(business_id_data)\n",
    "        neighbourhood = business_id_data[len(business_id_data) -2]\n",
    "        business_id_data[len(business_id_data) -2] = 1 if neighbourhood != None and neighbourhood != \"\" else 0\n",
    "        categories = business_id_data[-1]\n",
    "        if categories == None:\n",
    "            categories = ''\n",
    "        encoded_category = get_encoded_category(categories.split(\",\"))\n",
    "        business_id_data = business_id_data[:-1]\n",
    "        business_id_data += tuple(encoded_category)\n",
    "        #\n",
    "        if business_id in tips_count_business:\n",
    "            business_id_data.append(tips_count_business[business_id])\n",
    "        else :\n",
    "            business_id_data.append(0)\n",
    "            \n",
    "        if business_id in checkin_count_business:\n",
    "            business_id_data.append(checkin_count_business[business_id])\n",
    "        else:\n",
    "            business_id_data.append(0)\n",
    "        \n",
    "        '''\n",
    "        if business_id in photo_count_business:\n",
    "            business_id_data.append(photo_count_business[business_id])\n",
    "        else:\n",
    "            business_id_data.append(0)\n",
    "        '''\n",
    "        if business_id in good_count:\n",
    "            business_id_data.extend([good_count[business_id]])\n",
    "        else:\n",
    "            business_id_data.extend([0])\n",
    "            \n",
    "        if business_id in bad_count:\n",
    "            business_id_data.extend([bad_count[business_id]])\n",
    "        else:\n",
    "            business_id_data.extend([0])\n",
    "            \n",
    "        if business_id in business_attr:\n",
    "            business_id_data.extend(business_attr[business_id])\n",
    "        else :\n",
    "            business_id_data.extend([0 for i in range(38)])\n",
    "            \n",
    "        key = (user_id, business_id)\n",
    "        if key in tips_sentiment:\n",
    "            business_id_data.extend(tips_sentiment[key])\n",
    "        else:\n",
    "            business_id_data.extend([0,0,0,0])\n",
    "        '''\n",
    "        if business_id in business_sentiment:\n",
    "            business_id_data.extend(business_sentiment[business_id])\n",
    "        else:\n",
    "            business_id_data.extend([0,0])\n",
    "         \n",
    "            \n",
    "        ''' \n",
    "            \n",
    "        '''\n",
    "        if key in review_sentiment:\n",
    "            business_id_data.extend(review_sentiment[key])\n",
    "        else:\n",
    "            business_id_data.extend([0,0,0,0])\n",
    "        if key in review_good_count:\n",
    "            business_id_data.append(review_good_count[key])\n",
    "        else:\n",
    "            business_id_data.append(0)\n",
    "            \n",
    "        if key in review_bad_count:\n",
    "            business_id_data.append(review_bad_count[key])\n",
    "        else:\n",
    "            business_id_data.append(0)\n",
    "        '''\n",
    "        \n",
    "    else:\n",
    "        print(\"business not found!!\")\n",
    "    \n",
    "    final_data.extend(user_id_data)\n",
    "    final_data.extend(business_id_data)\n",
    "    \n",
    "    if len(data) == 3:\n",
    "        final_data.append(data[-1])\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def get_encoded_category(category_list):\n",
    "    category_list = [item.lstrip() for item in category_list]\n",
    "    final = []\n",
    "    yelp_cat_set = set(category_list)\n",
    "    for i in range(len(yelp_categories)):\n",
    "        if yelp_categories[i] in yelp_cat_set:\n",
    "            final.append(1)\n",
    "        else:\n",
    "            final.append(0)\n",
    "    return final\n",
    "\n",
    "def get_checkin_count(data):\n",
    "    business_id = data['business_id']\n",
    "    time = data['time']\n",
    "    count = 0\n",
    "    for i in time.values():\n",
    "        count += i\n",
    "    return (business_id, count)\n",
    "\n",
    "def get_sentiment(data):\n",
    "    global analyzer\n",
    "    text = data[1]\n",
    "    sentiment_dict = analyzer.polarity_scores(text)\n",
    "    result = list(sentiment_dict.values())\n",
    "    return (data[0],result)\n",
    "\n",
    "def get_avg_sentiment(data):\n",
    "    #print(data)\n",
    "    _id = data[0]\n",
    "    result = [0,0,0,0]\n",
    "    for sentiment in data[1]:\n",
    "        result[0] += sentiment[0]\n",
    "        result[1] += sentiment[1]\n",
    "        result[2] += sentiment[2]\n",
    "        result[3] += sentiment[3]\n",
    "        \n",
    "    #result = [item/len(data[1]) for item in result]\n",
    "    return (_id, result)\n",
    "\n",
    "def compute_rmse(pred_data,data_type):\n",
    "\n",
    "    file_2 = open(\"/Users/gopi/Desktop/Project/yelp_val.csv\", \"r\")\n",
    "    csv_2 = csv.reader(file_2, delimiter=\",\")\n",
    "    pred_2 = []\n",
    "    c = 0\n",
    "    actual_range = {0:0,1:0,2:0,3:0,4:0}\n",
    "    predict_range = {0:0,1:0,2:0,3:0,4:0}\n",
    "    for row in csv_2:\n",
    "        if c != 0:\n",
    "            pred_2.append(float(row[2]))\n",
    "        c+=1\n",
    "            \n",
    "    pred_1 = []\n",
    "    if data_type == \"CF\":\n",
    "        pred_1 = [data[2] for data in pred_data]\n",
    "    else:\n",
    "        pred_1 = pred_data\n",
    "        \n",
    "    total = 0\n",
    "    for i in range(len(pred_1)):\n",
    "        diff = abs(pred_1[i] - pred_2[i])\n",
    "        if diff >= 0 and diff < 1:\n",
    "            actual_range[0] += 1\n",
    "        elif diff >= 1 and diff < 2:\n",
    "            actual_range[1] += 1\n",
    "        elif diff >= 2 and diff < 3:\n",
    "            actual_range[2] += 1\n",
    "        elif diff >= 3 and diff < 4:\n",
    "            actual_range[3] += 1\n",
    "        else:\n",
    "            print(X_index[i][0], X_index[i][1])\n",
    "            actual_range[4] += 1\n",
    "        \n",
    "        total += ((pred_1[i] - pred_2[i]) ** 2)\n",
    "    \n",
    "    print(\"actual range is:\", actual_range)\n",
    "    return math.sqrt(total/len(pred_1))\n",
    "\n",
    "def compute_cat_rmse(pred_data,data_type):\n",
    "    file_2 = open(\"/Users/gopi/Desktop/Project/yelp_val.csv\", \"r\")\n",
    "    csv_2 = csv.reader(file_2, delimiter=\",\")\n",
    "    pred_2 = []\n",
    "    ids = []\n",
    "    c = 0\n",
    "    for row in csv_2:\n",
    "        if c != 0:\n",
    "            pred_2.append(float(row[2]))\n",
    "            ids.append(row[1])\n",
    "        c+=1\n",
    "            \n",
    "    pred_1 = []\n",
    "    if data_type == \"CF\":\n",
    "        pred_1 = [data[2] for data in pred_data]\n",
    "    else:\n",
    "        pred_1 = pred_data\n",
    "    \n",
    "    rmse_list = [0 for i in range(23)]\n",
    "    result = []\n",
    "    for i in range(len(pred_1)):\n",
    "        index = business_cat_data[ids[i]]\n",
    "        rmse_list[index] += ((pred_1[i] - pred_2[i]) ** 2)\n",
    "    for i in range(23):\n",
    "        result.append(math.sqrt(rmse_list[i] / cat_counts[i]))\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_category(categories):\n",
    "    if categories == None:\n",
    "        return len(yelp_categories)\n",
    "    #print(categories)\n",
    "    cat_list = categories.split(\",\")\n",
    "    category = ''\n",
    "    for cat in cat_list:\n",
    "        if cat.lstrip() in yelp_categories:\n",
    "            category = cat.lstrip()\n",
    "            break\n",
    "            \n",
    "    index = len(yelp_categories) if category == '' else cat_map[category]\n",
    "    return index\n",
    "\n",
    "def count(business_id):\n",
    "    return business_cat_data[business_id]\n",
    "\n",
    "def get_attributes(data):\n",
    "    if data == None:\n",
    "        return [0 for i in range(38)]\n",
    "    wifi = 1 if 'WiFi' in data and data['WiFi'] == 'free' else 0\n",
    "    price_range = int(data['RestaurantsPriceRange2']) if 'RestaurantsPriceRange2' in data else 0\n",
    "    credit_cards = 1 if 'BusinessAcceptsCreditCards' in data and bool(data['BusinessAcceptsCreditCards']) else 0\n",
    "    good_for_kids = 1 if 'GoodForKids' in data and bool(data['GoodForKids']) == True else 0\n",
    "    #caters = 1 if 'Caters' in data and bool(data['Caters']) else 0\n",
    "    noise_level = data['NoiseLevel'] if 'NoiseLevel' in data else ''\n",
    "    wh_acc = 1 if 'WheelchairAccessible' in data and bool(data['WheelchairAccessible']) == True else 0\n",
    "    good_for_grps = 1 if 'RestaurantsGoodForGroups' in data and bool(data['RestaurantsGoodForGroups']) == True else 0\n",
    "    wh_acc = 1 if 'WheelchairAccessible' in data and bool(data['WheelchairAccessible']) == True else 0\n",
    "    res_takeout = 1 if 'RestaurantsTakeOut' in data and bool(data['RestaurantsTakeOut']) == True else 0\n",
    "    res_res = 1 if 'RestaurantsReservations' in data and bool(data['RestaurantsReservations']) == True else 0\n",
    "    res_del = 1 if 'RestaurantsDelivery' in data and bool(data['RestaurantsDelivery']) == True else 0\n",
    "    by_app_only = 1 if 'ByAppointmentOnly' in data and bool(data['ByAppointmentOnly']) == True else 0\n",
    "    bp = 1 if 'BikeParking' in data and bool(data['BikeParking']) == True else 0\n",
    "    alcohol = 1 if 'Alcohol' in data and data['Alcohol'] != \"none\" else 0\n",
    "    res_ts = 1 if 'RestaurantsTableService' in data and bool(data['RestaurantsTableService']) == True else 0\n",
    "    has_tv = 1 if 'HasTV' in data and bool(data['HasTV']) else 0\n",
    "    out_seating = 1 if 'OutdoorSeating' in data and bool(data['OutdoorSeating']) else 0\n",
    "    res_attire = data['RestaurantsAttire'] if 'RestaurantsAttire' in data else ''\n",
    "    if res_attire == 'casual':\n",
    "        res_attire = 1\n",
    "    elif res_attire == 'dressy':\n",
    "        res_attire = 2\n",
    "    elif res_attire == 'formal':\n",
    "        res_attire = 3\n",
    "    else:\n",
    "        res_attire = 0\n",
    "    #dogs_allowed = 1 if 'DogsAllowed' in data and bool(data['DogsAllowed']) else 0\n",
    "    #accept_bc = 1 if 'BusinessAcceptsBitcoin' in data and bool(data['BusinessAcceptsBitcoin']) else 0\n",
    "    #byob = 1 if 'BYOB' in data and bool(data['BYOB']) else 0\n",
    "    #byob_cork = 1 if 'BYOBCorkage' in data and data['BYOBCorkage'] != \"no\" else 0\n",
    "    \n",
    "    g_f_m_map = data['GoodForMeal'] if 'GoodForMeal' in data else ''\n",
    "    g_f_m = []\n",
    "    if len(g_f_m_map) > 0:\n",
    "        g_f_m_map = g_f_m_map.split(\",\")\n",
    "        for value in g_f_m_map:\n",
    "            if value != None and \"True\" in value:\n",
    "                g_f_m.append(1)\n",
    "            else:\n",
    "                g_f_m.append(0)\n",
    "    else:\n",
    "        g_f_m = [0 for i in range(6)]\n",
    "    if(len(g_f_m) < 6):\n",
    "        for i in range(6-len(g_f_m)):\n",
    "            g_f_m.append(0)\n",
    "    elif len(g_f_m) > 6:\n",
    "        g_f_m = g_f_m[:6]   \n",
    "    \n",
    "        \n",
    "    bus_par_map = data['Ambience'] if 'Ambience' in data else ''\n",
    "    bus_par = []\n",
    "    if len(bus_par_map) > 0:\n",
    "        bus_par_map = bus_par_map.split(\",\")\n",
    "        for value in bus_par_map:\n",
    "            if value != None and \"True\" in value:\n",
    "                bus_par.append(1)\n",
    "            else:\n",
    "                bus_par.append(0)\n",
    "    else:\n",
    "        bus_par = [0 for i in range(8)]\n",
    "    if(len(bus_par) < 8):\n",
    "        for i in range(8-len(bus_par)):\n",
    "            bus_par.append(0)\n",
    "    elif len(bus_par) > 8:\n",
    "        bus_par = bus_par[:8]\n",
    "        \n",
    "        \n",
    "    best_nights_map = data['BestNights'] if 'BestNights' in data else ''\n",
    "    best_nights = []\n",
    "    if len(best_nights_map) > 0:\n",
    "        best_nights_map = best_nights_map.split(\",\")\n",
    "        for value in best_nights_map:\n",
    "            if value != None and \"True\" in value:\n",
    "                best_nights.append(1)\n",
    "            else:\n",
    "                best_nights.append(0)\n",
    "    else:\n",
    "        best_nights = [0 for i in range(7)]\n",
    "        \n",
    "    if(len(best_nights) < 7):\n",
    "        for i in range(7-len(best_nights)):\n",
    "            best_nights.append(0)\n",
    "    elif len(best_nights) > 7:\n",
    "        best_nights = best_nights[:7]\n",
    "\n",
    "    \n",
    "    if noise_level == \"very_loud\":\n",
    "        noise_level = -1\n",
    "    elif noise_level == 'loud':\n",
    "        noise_level = 0\n",
    "    elif noise_level == 'average':\n",
    "        noise_level = 3\n",
    "    else:\n",
    "        noise_level = 5\n",
    "        \n",
    "    final = [wifi, price_range, credit_cards, good_for_kids, noise_level, wh_acc, good_for_grps, res_takeout, res_res, res_del, by_app_only, bp, alcohol, res_ts, has_tv, out_seating, res_attire]\n",
    "    final.extend(g_f_m)\n",
    "    final.extend(bus_par)\n",
    "    final.extend(best_nights)\n",
    "    return final\n",
    "\n",
    "def write_data_to_file(output_file_path, data, predictions):\n",
    "    file = open(output_file_path, 'w')\n",
    "    file.write(\"user_id, business_id, prediction\")\n",
    "    file.write(\"\\n\")\n",
    "    for i in range(len(data)):\n",
    "        key = data[i]\n",
    "        rating = predictions[i]\n",
    "        output = key[0] + \",\" + key[1] + \",\" + str(rating)\n",
    "        file.write(output)\n",
    "        file.write(\"\\n\")\n",
    "    file.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1830ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Model based predictions\n",
    "\n",
    "\n",
    "initial_rdd = sc.textFile(input_folder_path + \"review_train.json\")\n",
    "initial_rdd = initial_rdd.map(lambda line : json.loads(line)).map(lambda line: (line[\"user_id\"], line[\"business_id\"], float(line[\"stars\"])))\n",
    "\n",
    "photo_count_business = sc.textFile(\"photo.json\").map(lambda x : json.loads(x)).map(lambda x : (x['business_id'], 1)).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1]))).collectAsMap()\n",
    "tips_count_business = sc.textFile(\"tip.json\").map(lambda x : json.loads(x)).map(lambda x : (x['business_id'], 1)).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1]))).collectAsMap()\n",
    "checkin_count_business = sc.textFile(\"checkin.json\").map(lambda x : json.loads(x)).map(lambda x : get_checkin_count(x)).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1]))).collectAsMap()\n",
    "\n",
    "#review_length_avg = sc.textFile(\"review_train.json\").map(lambda x : json.loads(x)).map(lambda x : (x['user_id'], len(x['text']))).groupByKey().mapValues(list).map(lambda x : (x[0], sum(x[1]) / len(x[1]))).collectAsMap()\n",
    "\n",
    "user_data = sc.textFile(\"user.json\").map(lambda x : json.loads(x)).map(lambda user : load_user_data(user)).map(lambda x : (x[0],(x[1:]))).collectAsMap()\n",
    "business_data = sc.textFile(\"business.json\").map(lambda x : json.loads(x)).map(lambda user : load_business_data(user)).map(lambda x : (x[0],(x[1:]))).collectAsMap()\n",
    "business_ids = set(business_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97d866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ba1230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_good_count(data):\n",
    "    good = 0\n",
    "    bad = 0\n",
    "    adj = [\"amazing\",\"good\",\"great\", \"excellent\", \"super\", \"nice\", \"awesome\", \"better\", \"impressive\", \"fabulous\", \"authentic\", \"helpful\", \"friendly\", \"fantastic\", \"highly\", \"best\",\"recommend\", \"delicious\", \"quality\", \"yum\", \"happy\"]\n",
    "    text = data[1].lower()\n",
    "    for a in adj:\n",
    "        if a in text:\n",
    "            good += 1\n",
    "    return (data[0],good)\n",
    "\n",
    "def get_bad_count(data):\n",
    "    bad = 0\n",
    "    bad_adj = [\"worst\",\"bad\", \"terrible\", \"awful\", \"rude\", \"disgusting\", \"dread\", \"dreadful\", \"appaling\", \"lousy\", \"poor\", \"poorly\", \"sad\", \"stinky\", \"stinking\", \"stinks\", \"stink\", \"gross\", \"grossed\", \"defective\", \"disappointed\", \"flies\"] \n",
    "    text = data[1].lower()\n",
    "    for b in bad_adj:\n",
    "        if b in text:\n",
    "            bad += 1\n",
    "    return (data[0],bad)\n",
    "\n",
    "business_cat_data= sc.textFile(\"business.json\").map(lambda x : json.loads(x)).map(lambda x : (x['business_id'], x['categories'])).mapValues(lambda x : get_category(x)).collectAsMap()\n",
    "cat_counts = sc.textFile(\"review_train.json\").map(lambda x : json.loads(x)).map(lambda x: (x['business_id'], x['user_id'])).map(lambda x : count(x[0])).map(lambda x : (x,1)).groupByKey().mapValues(list).mapValues(lambda x : sum(x)).sortBy(lambda x : x[0]).collectAsMap()\n",
    "good_count = sc.textFile(\"tip.json\").map(lambda x : json.loads(x)).map(lambda x : (x['business_id'], x['text'])).map(lambda x : get_good_count(x)).map(lambda x : (x[0], x[1])).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]))).collectAsMap()\n",
    "bad_count = sc.textFile(\"tip.json\").map(lambda x : json.loads(x)).map(lambda x : (x['business_id'], x['text'])).map(lambda x : get_bad_count(x)).map(lambda x : (x[0], x[1])).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]))).collectAsMap()\n",
    "#review_good_count = sc.textFile(\"review_train.json\").map(lambda x : json.loads(x)).map(lambda x : ((x['user_id'], x['business_id']), x['text'])).map(lambda x : get_good_count(x)).map(lambda x : (x[0], x[1])).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]))).collectAsMap()\n",
    "#review_bad_count = sc.textFile(\"review_train.json\").map(lambda x : json.loads(x)).map(lambda x : ((x['user_id'], x['business_id']), x['text'])).map(lambda x : get_bad_count(x)).map(lambda x : (x[0], x[1])).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]))).collectAsMap()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bfec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.149083137512207\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tips_sentiment = sc.textFile(\"tip.json\").map(lambda x : json.loads(x)).map(lambda x : ((x['user_id'], x['business_id']),x['text'])).map(lambda x: get_sentiment(x)).collectAsMap()\n",
    "#tips_bus_sem = tips_sentiment.map(lambda x : (x[0][1], x[1])).groupByKey().mapValues(list).map(lambda x : get_avg_sentiment(x)).collectAsMap()\n",
    "#tips_user_sem = tips_sentiment.map(lambda x : (x[0][0], x[1])).groupByKey().mapValues(list).map(lambda x : get_avg_sentiment(x)).collectAsMap()\n",
    "#review_sentiment = sc.textFile(\"review_train.json\").map(lambda x : json.loads(x)).map(lambda x : ((x['user_id'], x['business_id']), x['text'])).map(lambda x: get_sentiment(x)).collectAsMap()\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a04810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load X,Y is: 308.7852668762207\n"
     ]
    }
   ],
   "source": [
    "business_attr = sc.textFile(\"business.json\").map(lambda x : json.loads(x)).map(lambda x : (x['business_id'], x['attributes'])).mapValues(get_attributes).collectAsMap()\n",
    "start_1 = time.time()\n",
    "X_rdd = initial_rdd.filter(lambda x : x != None).map(lambda x : get_final_data(x))\n",
    "#print(X_rdd.take(5))\n",
    "X = X_rdd.map(lambda x : x[:-1]).collect()\n",
    "Y = X_rdd.map(lambda x : x[-1]).collect()\n",
    "testing_rdd = sc.textFile(\"/Users/gopi/Desktop/Project/yelp_val.csv\")\n",
    "testing_header = testing_rdd.first()\n",
    "testing_data_map = testing_rdd.filter(lambda x : x!= testing_header).map(lambda line : line.split(\",\")).map(lambda line: ((line[0], line[1]),\"\")).collectAsMap()\n",
    "\n",
    "X_test = []\n",
    "X_index = []\n",
    "business_id_set_test = set()\n",
    "for key in list(testing_data_map.keys()):\n",
    "    business_id_set_test.add(key[1])\n",
    "    X_test.append(get_final_data(key))\n",
    "    X_index.append(key)\n",
    "\n",
    "'''\n",
    "for key,value in user_business_rating_map.items():\n",
    "    X.append(get_final_data(key))\n",
    "    Y.append(value)\n",
    "'''\n",
    "    \n",
    "print(\"Time taken to load X,Y is:\", time.time() - start_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1c9ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('X', 400.0), (' Z', 200.0)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"1.txt\").filter(lambda x : 'hello' in x).collect()\n",
    "rdd_1 = sc.textFile(\"country.csv\").map(lambda x : x.split(\",\")).map(lambda x : (x[1],float(x[3]))).filter(lambda x : x[1] > 10000).groupByKey().mapValues(list).mapValues(lambda x : max(x)).collect()\n",
    "print(rdd_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91baeca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dRLqHBA0Z-jnlTvWJ4aoZQ d0qktSyIFR6AlR9kyXqA8g\n",
      "actual range is: {0: 102517, 1: 32695, 2: 6086, 3: 745, 4: 1}\n",
      "xgboost rmse is: 0.9741062470639191\n"
     ]
    }
   ],
   "source": [
    "xg_n_estimators = [250]\n",
    "xg_learning_rate = [0.16]\n",
    "xg_max_depth = [5]\n",
    "\n",
    "for i in range(len(xg_n_estimators)):\n",
    "    xgboost_model = xgb.XGBRegressor(n_estimators = xg_n_estimators[i], max_depth = xg_max_depth[i], learning_rate = xg_learning_rate[i])\n",
    "    xgboost_model.fit(X,Y)\n",
    "    predictions = xgboost_model.predict(X_test)\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 5.0:\n",
    "            predictions[i] = 5.0\n",
    "        if predictions[i] < 1:\n",
    "            predictions[i] = 1.0\n",
    "\n",
    "        \n",
    "    #predictions[i] = math.floor(predictions[i])\n",
    "    #pred_lr[i] = math.floor(pred_lr[i])\n",
    "\n",
    "    print(\"xgboost rmse is:\", compute_rmse(predictions, \"model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "write_data_to_file(output_file_path, X_index, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5922e6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual range is: {0: 102570, 1: 32633, 2: 6083, 3: 758, 4: 0}\n",
      "xgboost rmse is: 0.9738056179259277\n",
      "[0.00432289 0.0977509  0.00352983 0.00400111 0.00407389 0.00417507\n",
      " 0.00366203 0.00408729 0.00386539 0.00461947 0.00531972 0.00592296\n",
      " 0.00447216 0.         0.00371616 0.33984336 0.00486238 0.0056343\n",
      " 0.00995975 0.00409073 0.00200409 0.00375942 0.00796523 0.00426353\n",
      " 0.00360385 0.00408428 0.00601467 0.00410307 0.00424875 0.00420407\n",
      " 0.00344584 0.00434967 0.00468272 0.00321691 0.00420305 0.\n",
      " 0.00486347 0.00298845 0.00269764 0.00314097 0.0051004  0.00170246\n",
      " 0.02376023 0.00340164 0.00420758 0.00532422 0.00438506 0.00471826\n",
      " 0.00489518 0.01371484 0.00287112 0.00464894 0.00312182 0.00504023\n",
      " 0.02215073 0.01798684 0.03262547 0.01747278 0.00394538 0.00607192\n",
      " 0.01360019 0.01202593 0.00380454 0.05604648 0.00430456 0.00325057\n",
      " 0.00994064 0.00309802 0.00375381 0.00459894 0.00467554 0.00203807\n",
      " 0.00413311 0.00298644 0.00392999 0.00303682 0.00261134 0.00400372\n",
      " 0.00433168 0.00330499 0.00295406 0.00378394 0.00373316 0.00388961\n",
      " 0.00363743 0.00575193 0.01119653 0.01616195 0.01652192]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(\"xgboost rmse is:\", compute_rmse(predictions, \"model\"))\n",
    "print(xgboost_model.feature_importances_)\n",
    "print(xgboost_model.max_depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
